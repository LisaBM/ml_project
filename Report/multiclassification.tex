As mentioned in the beginning Support Vector Machines are binary classifiers with two possible output labels. Often one chooses them to be $1$ and $-1$. Our problem however was to classify unknown data into one out of ten different categories. Since there is no direct way to change one support vector machine so that it can choose from more than two labels we had to find a way to combine several SVMs, who together would be able to do so. There are a number of well known strategies tackling this issue. Let us first look at a very intuitive but also primitive approach called One-vs-All classification. Using this method one trains as many SVMs as there are classes, so ten in our case, and sets labels such that the $i$-th SVM has all training data with label $i$ set to 1 and everything else to $-1$. When trying to predict a new label, one simply looks for a classifier that gave this data point the label 1. Unfortunately, this approach has some issues. First of all, new data points are generally not uniquely classified. Therefore, it is necessary to add some sort of confidence score that signals which label is considered to be the most likely out of the possible ones. Finding good ways of assigning reliable confidence scores is often not an easy task. We decided to resolve this issue by computing the barycenters of each class, and when ambiguities arose, we chose the label of the closest barycenter out of the positive labels. This simple strategy improved our testing results. We decided on not devoting any more time on implementing a different strategy to determine confidence scores because the One-vs-All classification has another major drawback. The number of training points for each classifier being assigned positive and negative labels is very unbalanced, in our case on average $1$ to $9$. Therefore, one is likely to get shifted margins to what one would normally perceive to be correct. Since the negative class is simply greater in size, the penalty term will draw more attention towards avoiding points with negative labels inside the margin and henceforth neglecting the positive labels.\\ 

Another common intuitive approach would be the One-vs-One classification. Here, during training one only ever considers the data corresponding to two classes and sets one label to $1$ and the other one to $-1$ for every possible pair. 
In our case this would mean training $\binom{10}{2} =45$ different SVMs however with data sets of a fifth of the original size. Because in classification algorithm the issue of how to deal with ambiguities gets even worse, we decided not to implement this approach and instead focus our attention on a different strategy that seemed like a much better idea to us. And indeed, as the last chapter will reveal, this yielded very good results.

\subsection{Error-Correcting Output Codes}
Error-correcting output codes (ECOC) are easiest to explain considering a concrete example. We trained 15 different classifiers $f_i$. The rows of the table show what class is assigned which label depending on each classifier. For example $f_0$ assigns $1$ to all even numbers and $-1$ to all odd numbers. This way, each class has a string or code word consisting of $1$'s and $-1$'s it corresponds to.

\begin{table}[ht!]
	\centering
	\caption{Error Correcting Output Codes}
	\label{Codewords}
	\begin{tabular}{|c|r|r|r|r|r|r|r|r|r|r|r|r|r|r|r|}
		\hline
		Class	& $f_0$ & $f_1$ & $f_2$ & $f_3$ & $f_4$ & $f_5$ & $f_6$ & $f_7$ & $f_8$ & $f_9$ & $f_{10}$ & $f_{11}$ & $f_{12}$ & $f_{13}$ & $f_{14}$ \\ \hline \hline
		0	& 1 & 1 & -1 & -1 & -1 & -1 & 1 & -1 & 1 & -1 & -1 & 1 & 1 & -1 & 1 \\ \hline
		1	& -1 & -1 & 1 & 1 & 1 & 1 & -1 & 1 & -1 & 1 & 1 & -1 & -1 & 1 & -1 \\ \hline
		2	& 1 & -1 & -1 & 1 & -1 & -1 & -1 & 1 & 1 & 1 & 1 & -1 & 1 & -1 & 1 \\ \hline
		3	& -1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 & -1 & 1 \\ \hline
		4	& 1 & 1 & 1 & -1 & 1 & -1 & 1 & 1 & -1 & -1 & 1 & 1 & -1 & -1 & 1 \\ \hline
		5	& -1 & 1 & -1 & -1 & 1 & 1 & -1 & -1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 \\ \hline
		6	& 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 & -1 & 1 & -1 & -1 & 1 \\ \hline
		7	& -1 & -1 & -1 & 1 & 1 & 1 & 1 & -1 & 1 & -1 & 1 & 1 & -1 & -1 & 1 \\ \hline
		8	& 1 & 1 & -1 & 1 & -1 & 1 & 1 & -1 & -1 & 1 & -1 & -1 & -1 & 1 & 1 \\ \hline
		9	& -1 & 1 & 1 & 1 & -1 & -1 & -1 & -1 & 1 & -1 & 1 & -1 & -1 & 1 & 1 \\ \hline
	\end{tabular}
\end{table}  

The code words are chosen such that their Hamming distance (i.e.\ the number of entries where they differ) is maximized. In our case, each code word has a Hamming distance of at least seven to any of the others strings. Now, when one wants to label an unknown data point, each of the classifiers assigns it a label and one eventually gets an output string of 15 digits. We classify the data point according the code word it has the least Hamming distance to. The name \textit{Error Correcting Output Codes} is derived from the fact that for example any three classifiers can misclassify a data point and the algorithm still outputs the correct result. Depending on the difficulty of the problem one could choose more or less classifiers and hece in- or decrease the Hamming distance of the set of code words. We took this idea as well as the code words given above from \cite{dietterich1995solving}.


