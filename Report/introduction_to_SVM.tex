Given data points $\{x_i\}_{i=1}^l \subset \R^d$ with labels $\{y_i\}_{i=1}^l \subset \{\pm 1\}$ we want to seperate the different labels using a decision function of the form
$$f(x) = \sign\left(w^T\Phi(x) + b\right)$$ where $\Phi$ is the feature map as introduced in the lecture.
The Support Vector Machine (SVM) is a binary classifier that aims to find the maximum margin hyperplane in the feature space, i.e. the goal is to maximize the margin while softly penalizing points that lie on the wrong side of the boundary or inside the margin \cite{Bishop2006}. After the  dualization of this optimization problem we obtain the following equivalent quadratic program:
\begin{equation*}
\begin{aligned}
& \underset{0 \leq \alpha \leq C}{\text{maximize}}
& & \sum_{i = 1}^{l}\alpha_i - \frac{1}{2} \sum_{i, j = 1}^{l}\alpha_i  \alpha_j y_i y_j k(x_i, x_j)
\end{aligned}
\end{equation*}
with the dual variable $\alpha$, kernel function $k$ and penalty $C$.
After finding the optimal solution $\alpha^*$ of this program we can formulate the decision function as
$$f(x) = \sign\left(\sum_{i=1}^{l}\alpha^*_i y_i k(x_i,x_j)+b\right).$$
Note that only data points $x_i$ with $\alpha_i \neq 0$ appear in the decision funcion. Those points are called \textit{support vectors}. By analysing primal, dual and the corresponding KKT conditions we find that any $x_i$ with $\alpha_i = C$ lies exactly on the margin and any $x_i$ with $0<\alpha_i < C$ lies either inside the margin or on the wrong side of the margin. Additionally any data point $x_i$ on the margin must satisfy $y_i*f(x_i) = 1$. We can use this property to find the parameter $b$ \cite{Bishop2006}.